{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# from keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D,Dense\n",
    "# from tensorflow.keras.layers import Conv2D, UpSampling2D, GlobalAveragePooling2D, Dense,Flatten,Reshape,Input,MaxPooling2D\n",
    "# from tensorflow.keras.applications import ResNet50  # Assuming TensorFlow 2.x\n",
    "# from tensorflow.keras.optimizers import Adam  # Assuming TensorFlow 2.x\n",
    "# from tensorflow.keras.losses import SparseCategoricalCrossentropy  # Assuming TensorFlow 2.x\n",
    "# from tensorflow.keras.metrics import Accuracy\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 100\n",
    "# img_height = 224\n",
    "# img_width = 224\n",
    "# train_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#  r\"C:\\Users\\sarthak mohapatra\\Downloads\\crash\\data\\train\" ,\n",
    "#     image_size= (img_height, img_width),\n",
    "#     batch_size=batch_size\n",
    "\n",
    "# )\n",
    "\n",
    "# # test_set = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "# #     \"C:\\\\Users\\\\sarthak mohapatra\\\\Downloads\\\\archive\\\\data\\\\test\",\n",
    "# #     image_size= (img_height, img_width),\n",
    "# #     batch_size=batch_size)\n",
    "\n",
    "# val_set =  tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#  r\"C:\\Users\\sarthak mohapatra\\Downloads\\crash\\data\\val\",\n",
    "\n",
    "#     image_size= (img_height, img_width),\n",
    "#     batch_size=batch_size)\n",
    "# class_names = train_set.class_names\n",
    "\n",
    "# print(class_names)\n",
    "# AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# train = train_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# # test = test_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# img_shape = (img_height, img_width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 100\n",
    "# img_height = 224\n",
    "# img_width = 224\n",
    "\n",
    "# # Create image data generators with data augmentation for better generalization\n",
    "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#     rescale=1./255,  # Normalize pixel values to [0, 1]\n",
    "#     shear_range=0.2,  # Randomly apply shearing transformation (optional)\n",
    "#     zoom_range=0.2,  # Randomly zoom in/out (optional)\n",
    "#     horizontal_flip=True  # Randomly flip images horizontally\n",
    "# )\n",
    "\n",
    "# val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# # Create training and validation datasets with data augmentation\n",
    "# train_set = train_datagen.flow_from_directory(\n",
    "#     r\"C:\\Users\\sarthak mohapatra\\Downloads\\crash dataset - Copy\\data\\train\",\n",
    "#     target_size=(img_height, img_width),\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical'  # Specify categorical labels (one-hot encoded)\n",
    "# )\n",
    "\n",
    "# val_set = val_datagen.flow_from_directory(\n",
    "#     r\"C:\\Users\\sarthak mohapatra\\Downloads\\crash dataset - Copy\\data\\val\",\n",
    "#     target_size=(img_height, img_width),\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical'  # Specify categorical labels (one-hot encoded)\n",
    "# )\n",
    "# if hasattr(train_set, 'found_classes'):  # TensorFlow 2.2+\n",
    "#     class_names = train_set.found_classes.classes\n",
    "# else:  # Earlier TensorFlow versions\n",
    "#     train_data_dir = r\"C:\\Users\\sarthak mohapatra\\Downloads\\crash dataset - Copy\\data\\train\"\n",
    "#     class_names = os.listdir(train_data_dir)\n",
    "# # AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# # Preprocess and cache both training and validation datasets for performance\n",
    "# # train_set = train_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# # val_set = val_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# img_shape = (img_height, img_width, 3)\n",
    "# base_model = tf.keras.applications.ResNet50(input_shape=img_shape, include_top=False, weights='imagenet')\n",
    "# base_model.trainable = False  # Freeze base model layers\n",
    "# # Adjust the model's final layer\n",
    "# model = tf.keras.Sequential([\n",
    "#     base_model,\n",
    "#     UpSampling2D(size=(2, 2)),  \n",
    "#     Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "#     Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "#     Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "#     Flatten(),\n",
    "#     Dense(2, activation='sigmoid')  # Change activation to None\n",
    "# ])\n",
    "\n",
    "# # Change the loss function\n",
    "# model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_set, validation_data=val_set, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# def check_image_formats(dataset_dir):\n",
    "#     for root, dirs, files in os.walk(dataset_dir):\n",
    "#         for filename in files:\n",
    "#             if filename.endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp')):\n",
    "#                 try:\n",
    "#                     image_path = os.path.join(root, filename)\n",
    "#                     image = Image.open(image_path)\n",
    "#                     format = image.format\n",
    "#                     print(f\"File: {image_path}, Format: {format}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error reading {image_path}: {e}\")\n",
    "\n",
    "# dataset_dir = r\"C:\\Users\\sarthak mohapatra\\Downloads\\crash dataset - Copy\\data\\val\"\n",
    "# check_image_formats(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D,Dropout\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# img_shape = (224, 224, 3)\n",
    "\n",
    "# # Load the ResNet50 base model\n",
    "# base_model = tf.keras.applications.ResNet50(input_shape=img_shape, include_top=False, weights='imagenet')\n",
    "# base_model.trainable = False\n",
    "\n",
    "# x = base_model.layers[-1].output\n",
    "\n",
    "# # Add convolutional layers\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout\n",
    "# from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# img_shape = (224, 224, 3)\n",
    "\n",
    "# # Define the input tensor\n",
    "# # inputs = tf.keras.Input(shape=img_shape)\n",
    "\n",
    "\n",
    "# x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "# x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "# x = MaxPooling2D(pool_size=(2, 2))(x)  # Max pooling layer\n",
    "# x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "# x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "# # x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "# x = Dropout(rate=0.5)(x)\n",
    "\n",
    "# # Flatten the output of the convolutional layers\n",
    "# x = Flatten()(x)\n",
    "# # x=  Dense(512,activation='relu')(x)\n",
    "# # x=  Dense(128,activation='relu')(x)\n",
    "# # Add classification layers\n",
    "# outputs = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# # Create the model\n",
    "# model = Model(inputs=base_model.inputs, outputs=outputs)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(train_set, validation_data=val_set, epochs=5)\n",
    "\n",
    "# model = Model(inputs=t, outputs=output)\n",
    "# model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_set, validation_data=val_set, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('jim_crash.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = tf.keras.models.load_model(\"jim_crash.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label=[]\n",
    "# c=1\n",
    "# # cap=cv2.VideoCapture(r\"C:\\Users\\sarthak mohapatra\\Downloads\\Teens Involved in Car Crash That Was Like a Movie Scene.mp4\")\n",
    "# # while True:\n",
    "# #     ret, frame = cap.read()\n",
    "# #     if c%30==0:\n",
    "# #      print(c)\n",
    "# #     #ret, frame = cap.read()\n",
    "# #      resize_frame=cv2.resize(frame,(300,300))\n",
    "  \n",
    "# #      predict=predict_frame(resize_frame)\n",
    "# #      label.append(predict)\n",
    "# #     c+=1\n",
    "# #     cv2.imshow('frame',frame)\n",
    "# #     key=cv2.waitKey(1)&0xFF\n",
    "# #     if  key == ord(\"a\"):\n",
    "# #         break\n",
    "# # cap.release()\n",
    "\n",
    "# label=[]\n",
    "# c=1\n",
    "# cap=cv2.VideoCapture(0)\n",
    "# #cap= cv2.VideoCapture(\"C:\\\\Users\\\\sarthak mohapatra\\\\Downloads\\\\car vs bike accident cctv video in pollachi.mp4\")\n",
    "# # if not cap.isOpened():\n",
    "# #     print(\"Error: Could not open video file.\")\n",
    "# #     exit()\n",
    "# # while True:\n",
    "# #     grabbed, frame = cap.read()\n",
    "  \n",
    "# #     if not grabbed:\n",
    "# #       print(\"Warning: Failed to read frame from video file.\")  \n",
    "# #       break\n",
    "# #     if c%30==0:\n",
    "# #     #  if frame is not None: \n",
    "             \n",
    "# #           #resized_frame=tf.keras.preprocessing.image.smart_resize(frame, (300, 300), interpolation='bilinear')\n",
    "# #         # image.append(frame)\n",
    "# #      label.append(predict_frame(frame))\n",
    "# #         # if(len(image)==75):\n",
    "# #     cv2.imshow('frame',frame)    #     break\n",
    "# #     c+=1\n",
    "#     # cap.release()\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if c%30==0:\n",
    "#      print(c)\n",
    "#      #resize_frame=tf.keras.preprocessing.image.smart_resize(frame,(300,300),interpolation='bilinear')\n",
    "#      predict=predict_frame(frame)\n",
    "#      label.append(predict)\n",
    "#      c+=1\n",
    "#     cv2.imshow('frame',frame)\n",
    "#     key=cv2.waitKey(1)&0xFF\n",
    "#     if  key == ord(\"a\"):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "# # serialize model to JSON\n",
    "# import tensorflow.keras.backend as K\n",
    "# K.clear_session()\n",
    "# from keras.models import model_from_json\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"model.weights.h5\")\n",
    "# print(\"Saved model to disk\")\n",
    " \n",
    "# # later...\n",
    " \n",
    "# # load json and create model\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.weights.h5\")\n",
    "# print(\"Loaded model from disk\")\n",
    "# loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# def preprocess_image(image):\n",
    "#     # Resize image to 224x224\n",
    "#     img = cv2.resize(image, (224, 224))\n",
    "    \n",
    "#     # Convert color format to RGB\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     # Normalize pixel values\n",
    "#     img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "#     return img\n",
    "\n",
    "# def predict_frame(frame):\n",
    "#     # Preprocess the image for ResNet50\n",
    "#     frame = preprocess_image(frame)\n",
    "    \n",
    "#     # Predict using the model\n",
    "#     prediction = model.predict(np.expand_dims(frame, axis=0))\n",
    "#     predict_class = np.argmax(prediction, axis=1)[0]\n",
    "    \n",
    "#     return predict_class\n",
    "\n",
    "# label = []\n",
    "# cap = cv2.imread(r)\n",
    "# predict = predict_frame(cap)\n",
    "# label.append(predict)\n",
    "# print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from os import *\n",
    "import random\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation, BatchNormalization, Conv2D, MaxPooling2D ,ZeroPadding2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD , Adam , Adadelta\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.regularizers import *\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width,img_height = 300,300\n",
    "\n",
    "training_size = 600\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "if K.image_data_format()=='channels_first':\n",
    "    input_shape = (3,img_width,img_height)\n",
    "else:\n",
    "    input_shape = (img_width,img_height,3)\n",
    "train =  r\"C:\\Users\\sarthak mohapatra\\Downloads\\crash\\data\\train\"\n",
    "val = r\"C:\\Users\\sarthak mohapatra\\Downloads\\crash\\data\\val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 791 images belonging to 2 classes.\n",
      "Found 98 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accident': 0, 'Non Accident': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                rescale = 1./255.,\n",
    "            \n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "            train,\n",
    "            target_size = (img_width,img_height),\n",
    "            batch_size = batch_size,\n",
    "            color_mode = 'rgb',\n",
    "            class_mode = 'binary'\n",
    ")\n",
    "\n",
    "val_datagen =  ImageDataGenerator(\n",
    "                rescale = 1./255.\n",
    ")\n",
    "validation_gen = val_datagen.flow_from_directory(\n",
    "                 val,\n",
    "                 target_size = (img_width,img_height),\n",
    "                 batch_size = batch_size,\n",
    "                 color_mode = 'rgb',\n",
    "                 class_mode = 'binary')\n",
    "train_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "MyCnn = tf.keras.models.Sequential([\n",
    "  layers.BatchNormalization(),\n",
    "  layers.Conv2D(32, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(128, 3, activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  layers.Dense(2, activation= 'softmax')\n",
    "])\n",
    "\n",
    "MyCnn.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarthak mohapatra\\anaconda3\\envs\\sarthak\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.4764 - loss: 8.9323 - val_accuracy: 0.4796 - val_loss: 0.5583\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarthak mohapatra\\anaconda3\\envs\\sarthak\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.6875 - loss: 0.3242 - val_accuracy: 0.4694 - val_loss: 0.5636\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.6128 - loss: 0.6446 - val_accuracy: 0.6327 - val_loss: 0.5050\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.7812 - loss: 0.2774 - val_accuracy: 0.6327 - val_loss: 0.5043\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.7094 - loss: 0.5570 - val_accuracy: 0.7449 - val_loss: 0.4764\n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.7188 - loss: 0.2773 - val_accuracy: 0.7755 - val_loss: 0.4605\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.7955 - loss: 0.4322 - val_accuracy: 0.6735 - val_loss: 0.4773\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.8125 - loss: 0.2116 - val_accuracy: 0.7551 - val_loss: 0.4573\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8377 - loss: 0.3694 - val_accuracy: 0.6633 - val_loss: 0.4536\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.8438 - loss: 0.1872 - val_accuracy: 0.7653 - val_loss: 0.3699\n"
     ]
    }
   ],
   "source": [
    "history = MyCnn.fit(train_gen,\n",
    "                              steps_per_epoch = 24,\n",
    "                              validation_data = validation_gen ,\n",
    "                              validation_steps = 6,\n",
    "                              epochs = 10,\n",
    "                              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyCnn.save('jim_crash.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"jim_crash.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "[0.43826526 0.56173474]\n",
      "non accident\n",
      "[[0.43826526 0.56173474]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \n",
    "    img = cv2.resize(image, (300, 300))\n",
    "    \n",
    "  \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "  \n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "def predict_frame(frame):\n",
    "    # Preprocess the image for ResNet50\n",
    "    frame = preprocess_image(frame)\n",
    "    img_array = tf.keras.utils.img_to_array(frame)\n",
    "    # img_batch = np.expand_dims(img_array, axis=0)\n",
    "    # prediction=(model.predict(img_batch) < 0.5).astype(\"int32\")\n",
    "    # print(prediction)\n",
    "    # if(prediction[0][0]==0):\n",
    "    #     return(\"Accident Detected\")\n",
    "    # else:\n",
    "    #     return(\"No Accident\")\n",
    "    # Predict using the model\n",
    "    prediction = loaded_model.predict(np.expand_dims(img_array, axis=0))\n",
    "    # predict_class = np.argmax(prediction, axis=1)[0]\n",
    "    print(prediction[0])\n",
    "    if prediction[0][0]>0.7  :\n",
    "     print(\" Accident\")\n",
    "    elif prediction[0][0]>0.5 and prediction[0][1]>0.5 :\n",
    "      print('Accident')\n",
    "    else:\n",
    "       print('non accident')\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "label = []\n",
    "cap = cv2.imread(r\"C:\\Users\\sarthak mohapatra\\Downloads\\crash\\data\\test\\Non Accident\\5_23.jpg\")\n",
    "predict = predict_frame(cap)\n",
    "label.append(predict)\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label=[]\n",
    "# cap=cv2.imread(r)\n",
    "# cap=cv2.resize(cap,(224,224))\n",
    "# predict=predict_frame(cap)\n",
    "# label.append(predict)\n",
    "# print(label[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarthak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
